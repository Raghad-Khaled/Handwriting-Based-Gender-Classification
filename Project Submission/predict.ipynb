{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import cv2\n",
    "import os\n",
    "\"\"\"\n",
    "to install it on conda => https://anaconda.org/conda-forge/python-dotenv\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "#import commonfunctions as cf\n",
    "import sklearn\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.metrics import *\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import feature, exposure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def HistogramofOrientedGradients(image):\n",
    "    resized_img = resize(image, (64, 128))\n",
    "    fd, hog_image = feature.hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True)\n",
    "    \n",
    "    return (np.array(fd)).flatten()\n",
    "\n",
    "N_ANGLE_BINS = 40\n",
    "BIN_SIZE = 360 // N_ANGLE_BINS\n",
    "LEG_LENGTH = 25\n",
    "\n",
    "def get_contour_pixels(bw_image):\n",
    "    contours, _= cv2.findContours(\n",
    "        bw_image, cv2.RETR_TREE, \n",
    "        cv2.CHAIN_APPROX_NONE\n",
    "        ) \n",
    "    # contours = imutils.grab_contours(contours)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[1:]\n",
    "    \n",
    "    img2 = bw_image.copy()[:,:,np.newaxis]\n",
    "    img2 = np.concatenate([img2, img2, img2], axis = 2)\n",
    "    \n",
    "    # if self.show_images:\n",
    "    #     for cnt in contours : \n",
    "    #         cv2.drawContours(img2, [cnt], 0, (255, 0, 0), 1)  \n",
    "            \n",
    "    #     plt.imshow(img2, cmap='gray')\n",
    "    return contours\n",
    "\n",
    "def get_hinge_features(bw_image):\n",
    "    # if self.is_binary:\n",
    "    #     bw_image, _ = self.preprocess_binary_image(img_file, self.sharpness_factor, self.bordersize)\n",
    "    # else:\n",
    "    #     bw_image, _ = self.preprocess_image(img_file, self.sharpness_factor, self.bordersize)\n",
    "    \n",
    "    contours = get_contour_pixels(bw_image)\n",
    "    \n",
    "    hist = np.zeros((N_ANGLE_BINS, N_ANGLE_BINS))\n",
    "        \n",
    "    # print([len(cnt) for cnt in contours])\n",
    "    for cnt in contours:\n",
    "        n_pixels = len(cnt)\n",
    "        if n_pixels <= LEG_LENGTH:\n",
    "            continue\n",
    "        \n",
    "        points = np.array([point[0] for point in cnt])\n",
    "        xs, ys = points[:, 0], points[:, 1]\n",
    "        point_1s = np.array([cnt[(i + LEG_LENGTH) % n_pixels][0] for i in range(n_pixels)])\n",
    "        point_2s = np.array([cnt[(i - LEG_LENGTH) % n_pixels][0] for i in range(n_pixels)])\n",
    "        x1s, y1s = point_1s[:, 0], point_1s[:, 1]\n",
    "        x2s, y2s = point_2s[:, 0], point_2s[:, 1]\n",
    "        \n",
    "        phi_1s = np.degrees(np.arctan2(y1s - ys, x1s - xs) + np.pi)\n",
    "        phi_2s = np.degrees(np.arctan2(y2s - ys, x2s - xs) + np.pi)\n",
    "        \n",
    "        indices = np.where(phi_2s > phi_1s)[0]\n",
    "        \n",
    "        for i in indices:\n",
    "            phi1 = int(phi_1s[i] // BIN_SIZE) % N_ANGLE_BINS\n",
    "            phi2 = int(phi_2s[i] // BIN_SIZE) % N_ANGLE_BINS\n",
    "            hist[phi1, phi2] += 1\n",
    "            \n",
    "    normalised_hist = hist / np.sum(hist)\n",
    "    feature_vector = normalised_hist[np.triu_indices_from(normalised_hist, k = 1)]\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "#Initialization:\n",
    "N_RHO_BINS = 7\n",
    "N_ANGLE_BINS = 12\n",
    "N_BINS = N_RHO_BINS * N_ANGLE_BINS\n",
    "BIN_SIZE = 360 // N_ANGLE_BINS\n",
    "R_INNER = 5.0\n",
    "R_OUTER = 35.0\n",
    "K_S = np.arange(3, 8)\n",
    "def get_cold_features(bw_image, approx_poly_factor = 0.01 , sharpness_factor = 10 , bordersize = 3):\n",
    "\n",
    "    #bw_image, _ = preprocess_image(img_file, sharpness_factor, bordersize)\n",
    "    contours = get_contour_pixels(bw_image)\n",
    "    \n",
    "    rho_bins_edges = np.log10(np.linspace(R_INNER, R_OUTER, N_RHO_BINS))\n",
    "    feature_vectors = np.zeros((len(K_S), N_BINS))\n",
    "    \n",
    "    for j, k in enumerate(K_S):\n",
    "        hist = np.zeros((N_RHO_BINS, N_ANGLE_BINS))\n",
    "        for cnt in contours:\n",
    "            epsilon = approx_poly_factor * cv2.arcLength(cnt,True)\n",
    "            cnt = cv2.approxPolyDP(cnt,epsilon,True)\n",
    "            n_pixels = len(cnt)\n",
    "            \n",
    "            point_1s = np.array([point[0] for point in cnt])\n",
    "            x1s, y1s = point_1s[:, 0], point_1s[:, 1]\n",
    "            point_2s = np.array([cnt[(i + k) % n_pixels][0] for i in range(n_pixels)])\n",
    "            x2s, y2s = point_2s[:, 0], point_2s[:, 1]\n",
    "            \n",
    "            thetas = np.degrees(np.arctan2(y2s - y1s, x2s - x1s) + np.pi)\n",
    "            rhos = np.sqrt((y2s - y1s) ** 2 + (x2s - x1s) ** 2)\n",
    "            if(rhos.all()):\n",
    "                rhos_log_space = np.log10(rhos)\n",
    "            else:\n",
    "                rhos_log_space = rhos\n",
    "            \n",
    "            quantized_rhos = np.zeros(rhos.shape, dtype=int)\n",
    "            for i in range(N_RHO_BINS):\n",
    "                quantized_rhos += (rhos_log_space < rho_bins_edges[i])\n",
    "                \n",
    "            for i, r_bin in enumerate(quantized_rhos):\n",
    "                theta_bin = int(thetas[i] // BIN_SIZE) % N_ANGLE_BINS\n",
    "                hist[r_bin - 1, theta_bin] += 1\n",
    "            \n",
    "        normalised_hist = hist / hist.sum()\n",
    "        feature_vectors[j] = normalised_hist.flatten()\n",
    "        \n",
    "    return feature_vectors.flatten()\n",
    "\n",
    "def get_hinge_with_cold(bw_image):\n",
    "    hinge=get_hinge_features(bw_image)\n",
    "    cold=get_cold_features(bw_image)\n",
    "    return np.concatenate([hinge, cold])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# input files directories\n",
    "TEST_DIRECTORY = os.environ.get(\"TEST_DIRECTORY\")\n",
    "\n",
    "OUTPUT_DIRECTORY = os.environ.get(\"OUTPUT_DIRECTORY\") \n",
    "\n",
    "predicted_class = []\n",
    "time_list = []\n",
    "images = []\n",
    "# read images \n",
    "male_data=np.load('male_data_small.npy',allow_pickle=True)\n",
    "female_data = np.load('female_data_small.npy',allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "# for filename in os.listdir(TEST_DIRECTORY):\n",
    "#     image = cv2.imread(os.path.join(TEST_DIRECTORY,filename), 0)\n",
    "#     if image is not None:\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         k=10\n",
    "#         wshape=(100*k,142*k)\n",
    "\n",
    "#         image=cv2.resize(image,wshape)\n",
    "#         image = cv2.GaussianBlur(image,(3,3),cv2.BORDER_DEFAULT)\n",
    "\n",
    "#         image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "#                 cv2.THRESH_BINARY,11,2) # making the image binary\n",
    "#         image = cv2.medianBlur(image, 5)\n",
    "#         image = cv2.medianBlur(image, 5)\n",
    "#         image = cv2.erode(image, None, iterations = 1)\n",
    "#         images.append(image)\n",
    "\n",
    "#         # predicted_class.append() # here call preprocessing function and the model\n",
    "#         end_time = time.time() - start_time\n",
    "#         if(end_time < .001):\n",
    "#             time_list.append(.001)\n",
    "#         else:\n",
    "#             time_list.append(end_time)\n",
    "#images=np.array(images)\n",
    "images=np.concatenate((male_data,female_data))\n",
    "cold_f=[]\n",
    "hinge_f=[]\n",
    "hog_f=[]\n",
    "hinge_with_cold_f=[]\n",
    "\n",
    "\n",
    "for i,img in enumerate(images):\n",
    "    cold_f.append(get_cold_features(img))\n",
    "    hinge_f.append(get_hinge_features(img))\n",
    "    hog_f.append(HistogramofOrientedGradients(img))\n",
    "    hinge_with_cold_f.append(get_hinge_with_cold(img))\n",
    "\n",
    "cold_f=np.array(cold_f)\n",
    "hinge_f=np.array(hinge_f)\n",
    "hog_f=np.array(hog_f)\n",
    "hinge_with_cold_f=np.array(hinge_with_cold_f)\n",
    "\n",
    "#load models\n",
    "\n",
    "cold_model=pickle.load(open('cold','rb'))\n",
    "pca_cold=pickle.load(open('cold_pca','rb'))\n",
    "cold_f=pca_cold.transform(cold_f)\n",
    "y_cold_pred=cold_model.predict_proba(cold_f)\n",
    "#############################################\n",
    "hinge_model=pickle.load(open('hinge','rb'))\n",
    "y_hinge_pred=hinge_model.predict_proba(hinge_f)\n",
    "###############################################\n",
    "hog_model=pickle.load(open('hog','rb'))\n",
    "y_hog_pred=hog_model.predict_proba(hog_f)\n",
    "#################################################\n",
    "hinge_with_cold_model=pickle.load(open('hinge_with_cold','rb'))\n",
    "pca_hinge_with_cold=pickle.load(open('hinge_with_cold_pca','rb'))\n",
    "hinge_with_cold_f=pca_hinge_with_cold.transform(hinge_with_cold_f)\n",
    "y_hinge_with_cold_pred=hinge_with_cold_model.predict_proba(hinge_with_cold_f)\n",
    "##end of prediction \n",
    "\n",
    "\n",
    "y_pred=np.argmax(y_cold_pred+y_hinge_pred+y_hog_pred+y_hinge_with_cold_pred, axis=1)\n",
    "print(y_pred)\n",
    "y_test=np.concatenate((np.ones(male_data.shape[0]),\n",
    "                np.zeros(female_data.shape[0])),\n",
    "                axis=0)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# result_file = open(OUTPUT_DIRECTORY + '/results.txt', 'w')\n",
    "# time_file = open(OUTPUT_DIRECTORY + '/times.txt', 'w')\n",
    "# for i in range(len(time_list)):\n",
    "#     result_file.write(predicted_class[i] + '\\n')\n",
    "#     time_file.write(time[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16eaf64d804c64982ecaa3d552fe6459f2afbedbd70923f4efe6f78d1fdb2db7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
